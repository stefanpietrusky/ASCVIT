"""
title: ASCVIT V1 [AUTOMATIC STATISTICAL CALCULATION, VISUALIZATION AND INTERPRETATION TOOL]
author: stefanpietrusky
author_url: https://downchurch.studio/
version: 0.1
"""


import re
import subprocess

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px  
import plotly.graph_objects as go  
import seaborn as sns
from matplotlib.patches import Patch
from scipy import stats
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression, LogisticRegression
from statsmodels.stats.multicomp import pairwise_tukeyhsd

import streamlit as st


# Function for displaying data types and statistics
def display_data_info(df):
    st.write("**Data description:**")
    st.write(df.describe())
    st.write(f"**Number of data points:** {len(df)}")
    
    numerical_columns = df.select_dtypes(include=np.number).columns.tolist()
    categorical_columns = df.select_dtypes(include='object').columns.tolist()

    st.write("**Numerical variables:** ", ", ".join(numerical_columns))
    st.write("**Categorical variables:** ", ", ".join(categorical_columns))
    
    return numerical_columns, categorical_columns


# Function for performing descriptive statistics
def descriptive_statistics(df, numerical_columns):
    # Selection of the diagram type
    chart_type = st.selectbox("Select the diagram:", ["Histogram", "Boxplot", "Pairplot", "Correlation matrix"])
    
    # Show explanations based on the selection 
    if chart_type == "Histogram":
        st.markdown("""
        **Histogram:**
        A histogram shows the distribution of a numerical variable. It helps to 
        recognize how frequently certain values occur in the data and whether there are patterns, such as a normal distribution.
        """)
    elif chart_type == "Boxplot":
        st.markdown("""
        **Boxplot:**
        A boxplot shows the distribution of a numerical variable through its quartiles. 
        It helps to identify outliers and visualize the dispersion of the data.
        """)
    elif chart_type == "Pairplot":
        st.markdown("""
        **Pairplot:**
        A pairplot shows the relationships between different numerical variables through scatterplots.
        It helps to identify possible relationships between variables.
        """)
    elif chart_type == "Correlation matrix":
        st.markdown("""
        *Correlation matrix:**
        The correlation matrix shows the linear relationships between numerical variables.
        A positive correlation indicates that high values in one variable also correlate with high values in another.
        """)

    # Selection of variables
    if chart_type in ["Pairplot", "Correlation matrix"]:
        selected_vars = st.multiselect("Select variables:", numerical_columns, default=numerical_columns)
    else:
        selected_vars = [st.selectbox("Select a variable:", numerical_columns)]
    
    # Only show logarithmic scaling if it is not "correlation matrix"
    if chart_type != "Correlation matrix":
        apply_log_scale = st.checkbox("Apply logarithmic scaling?", value=False)
    else:
        apply_log_scale = False 
    
    # Button to start the analysis
    if st.button("Create diagram"):
        if chart_type == "Histogram":
            plot_histogram(df, selected_vars[0], apply_log_scale)
        elif chart_type == "Boxplot":
            plot_boxplot(df, selected_vars[0], apply_log_scale)
        elif chart_type == "Pairplot":
            plot_pairplot(df, selected_vars)
        elif chart_type == "Correlation matrix":
            plot_correlation_matrix(df, selected_vars)


# Function for displaying a histogram with LLM analysis
def plot_histogram(df, variable, apply_log_scale):
    # Cleaning the data (removing NaN values)
    cleaned_data = df[variable].dropna()

    # Calculating metrics
    mean_value = cleaned_data.mean()
    median_value = cleaned_data.median()
    std_value = cleaned_data.std()
    min_value = cleaned_data.min()
    max_value = cleaned_data.max()

    # Calculation of mean + and - standard deviation
    std_upper = mean_value + std_value
    std_lower = max(0, mean_value - std_value)  

    # Concentration range (mean Â± 1 standard deviation)
    concentration_range = (mean_value - std_value, mean_value + std_value)

    # Description of the scatter
    if std_value < (max_value - min_value) / 6:
        scatter = "low"
    elif std_value < (max_value - min_value) / 3:
        scatter = "moderate"
    else:
        scatter = "high"

    # Description of the distribution
    if abs(mean_value - median_value) < 0.1 * std_value:
        distribution = "symmetrical"
    elif mean_value > median_value:
        distribution = "right-skewed"
    else:
        distribution = "left-skewed"

    # Creating the histogram
    fig, ax = plt.subplots()
    ax.hist(cleaned_data, bins=30, edgecolor='black', alpha=0.7)

    # Add vertical lines for mean, median and standard deviation
    ax.axvline(mean_value, color='red', linestyle='--', label=f'Mean: {mean_value:.2f}')
    ax.axvline(median_value, color='green', linestyle='-', label=f'Median: {median_value:.2f}')
    ax.axvline(std_upper, color='blue', linestyle=':', label=f'+1 Std: {std_upper:.2f}')
    ax.axvline(std_lower, color='blue', linestyle=':', label=f'-1 Std: {std_lower:.2f}')

    # Title and legend
    ax.set_title(f"Histogram of {variable}")
    ax.legend(title=f'Std-Deviation: {std_value:.2f}')

    # Log scaling if desired
    if apply_log_scale:
        ax.set_yscale('log')

    # Display of the graphic in Streamlit
    st.pyplot(fig)

    # Providing a standardized structure for LLM analysis
    context = (
        f"Here is an analysis of the distribution of the variable '{variable}':\n"
        f"- Mean: {mean_value:.2f}\n"
        f"- Median: {median_value:.2f}\n"
        f"- Standard deviation: {std_value:.2f}\n"
        f"- Minimum: {min_value:.2f}\n"
        f"- Maximum: {max_value:.2f}\n\n"
        f"The distribution of the data shows a {distribution} distribution.\n"
        f"The small difference between mean and median indicates a {distribution} distribution.\n"
        f"A strong concentration of data points is observed between {concentration_range[0]:.2f} and {concentration_range[1]:.2f}.\n"
        f"The scatter of the data is described as {scatter}, indicating a relatively tight distribution around the mean.\n\n"
        f"Please analyze this distribution in the histogram, paying particular attention to symmetry, scatter, and potential deviations.\n"
        f"Avoid calling the distribution normal unless there are explicit indications.\n"
        f"Use only the names of the variables {variable} in the analysis!"
    )

    # Send request to the LLM
    response = query_llm_via_cli(context)
    st.write(f"**Histogram Interpretation:** {response}")


# Function for displaying a boxplot with LLM analysis
def plot_boxplot(df, variable, apply_log_scale):
    # Calculating metrics for the boxplot
    mean_value = df[variable].mean()
    median_value = df[variable].median()
    std_value = df[variable].std()
    q1 = df[variable].quantile(0.25)
    q3 = df[variable].quantile(0.75)
    iqr = q3 - q1
    lower_whisker = max(df[variable].min(), q1 - 1.5 * iqr)
    upper_whisker = min(df[variable].max(), q3 + 1.5 * iqr)

    # Creating the boxplot
    fig = px.box(df, y=variable)
    fig.update_layout(title=f"Boxplot of {variable}")

    if apply_log_scale:
        fig.update_yaxes(type="log")  
    
    st.plotly_chart(fig)

    # Context for submission to the LLM
    context = (
        f"Here is an analysis of the distribution of the variable '{variable}' based on a boxplot:\n"
        f"- Mean: {mean_value:.2f}\n"
        f"- Median: {median_value:.2f}\n"
        f"- Standard deviation: {std_value:.2f}\n"
        f"- Lower quartile (Q1): {q1:.2f}\n"
        f"- Upper quartile (Q3): {q3:.2f}\n"
        f"- Interquartile range (IQR): {iqr:.2f}\n"
        f"- Potential outliers outside values from {lower_whisker:.2f} to {upper_whisker:.2f}.\n"
        f"Please analyze this distribution and identify patterns or outliers.\n"
        f"Use only the names of the variables {variable} in the analysis!"
    )

    # Send request to the LLM
    response = query_llm_via_cli(context)
    st.write(f"**Boxplot Interpretation:** {response}")


# Function to calculate regression data
def calculate_regression_stats(df, selected_vars):
    regression_results = []
    for var1 in selected_vars:
        for var2 in selected_vars:
            if var1 != var2:
                # Remove NaN values from both variables to ensure that the number of data points matches
                non_nan_data = df[[var1, var2]].dropna()

                X = non_nan_data[[var1]].values.reshape(-1, 1)
                y = non_nan_data[var2].values

                # Perform the regression analysis only if both variables contain data after removing NaNs
                if len(X) > 0 and len(y) > 0:
                    model = LinearRegression()
                    model.fit(X, y)
                    r_squared = model.score(X, y)
                    slope = model.coef_[0]

                    regression_results.append((var1, var2, slope, r_squared))

    return regression_results


# Function to display a pairplot with LLM analysis
def plot_pairplot(df, selected_vars):
    if len(selected_vars) > 1:
        st.write("**Pairplot with regression lines:**")
        pairplot_fig = sns.pairplot(df[selected_vars], kind='reg', diag_kind='kde', 
                                    plot_kws={'line_kws': {'color': 'red'}, 'scatter_kws': {'color': 'blue'}})
        st.pyplot(pairplot_fig.fig)

        # Calculate the correlation matrix
        corr_matrix = df[selected_vars].corr()

        # Calculate regression statistics
        regression_stats = calculate_regression_stats(df, selected_vars)

        # Create a neutral context for the analysis
        correlation_list = "\n".join(
            [f"The correlation between {var1} and {var2} is {corr_matrix.at[var1, var2]:.2f}."
             for var1 in corr_matrix.columns for var2 in corr_matrix.columns if var1 != var2]
        )

        regression_list = "\n".join(
            [f"The regression line for {var1} and {var2} has a slope of {slope:.2f} and an RÂ² of {r_squared:.2f}."
             for var1, var2, slope, r_squared in regression_stats]
        )

        # Create a neutral context for the analysis
        context = (
            f"Here are the correlation and regression analyses between the selected variables:\n"
            f"{correlation_list}\n\n"
            f"{regression_list}\n\n"
            f"Please analyze these relationships in detail based solely on the numerical values (correlation and regression lines).\n"
            f"Use only the names of the variables {selected_vars} in the analysis!"
        )

        # Send request to the LLM
        response = query_llm_via_cli(context)
        st.write(f"**Pairplot Interpretation:** {response}")
    else:
        st.error("At least two variables must be selected for a pairplot.")


# Function to display the correlation matrix with LLM analysis
def plot_correlation_matrix(df, selected_vars):
    if len(selected_vars) > 1:
        corr_matrix = df[selected_vars].corr()

        # Display the correlation matrix
        fig, ax = plt.subplots()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)
        ax.set_title("Correlation Matrix")
        st.pyplot(fig)

        # Identify significant correlations
        high_correlations = []
        for var1 in corr_matrix.columns:
            for var2 in corr_matrix.columns:
                if var1 != var2 and abs(corr_matrix.at[var1, var2]) >= 0.5:  # Only significant correlations
                    if (var2, var1) not in [(v1, v2) for v1, v2, _ in high_correlations]:  # Avoid duplicates
                        high_correlations.append((var1, var2, corr_matrix.at[var1, var2]))

        # Create context for the LLM
        if high_correlations:
            correlation_list = "\n".join([f"- {var1} and {var2} have a correlation value of {value:.2f}, "
                                        f"indicating a {'strong' if abs(value) > 0.7 else 'moderate' if abs(value) > 0.5 else 'weak'} correlation."
                                        for var1, var2, value in high_correlations])

            context = (
                f"Here is an analysis of the significant correlations between the selected variables in the correlation matrix:\n"
                f"{correlation_list}\n\n"
                f"Please analyze the correlations solely based on their strength and significance.\n"
                f"Use only the names of the variables {selected_vars} in the analysis!"
                f"Focus in detail on the statistical relationship and patterns."
            )

            # Send request to the LLM
            response = query_llm_via_cli(context)
            st.write(f"**Model Response:** {response}")
        else:
            st.write("**No significant correlations were found.**")
    else:
        st.write("**The correlation matrix cannot be displayed because fewer than two variables were selected.**")


# Function for hypothesis tests
def hypothesis_testing(df, numerical_columns, categorical_columns):
    test_type = st.selectbox("Choose the hypothesis test:", ["t-Test", "ANOVA", "Chi-square test"])
    
    if test_type == "t-Test":
        st.markdown("""
        **t-Test:**
        The t-test compares the means of two groups to determine whether they are significantly different.
        A low p-value indicates that the difference between the groups cannot be explained by chance.
        """)
        t_test(df, numerical_columns, categorical_columns)
        
    elif test_type == "ANOVA":
        st.markdown("""
        **ANOVA (Analysis of Variance):**
        ANOVA tests whether there are significant differences in the means of several groups.
        If the p-value is small, this indicates differences between the groups.
        """)
        anova_test(df, numerical_columns, categorical_columns)
        
    elif test_type == "Chi-square test":
        st.markdown("""
        **Chi-square test:**
        This test checks whether there is a statistically significant association between two categorical variables.
        """)
        chi_square_test(df, categorical_columns)


# Function for the t-Test
def t_test(df, numerical_columns, categorical_columns):
    group_col = st.selectbox("Choose the group variable:", categorical_columns)
    value_col = st.selectbox("Choose the value variable:", numerical_columns)
    
    group1 = st.text_input("Name of group 1:")
    group2 = st.text_input("Name of group 2:")

    apply_log_scale = st.checkbox("Apply logarithmic scaling?", value=False)

    if st.button("Perform t-Test"):
        # Count original data points
        group1_data = df[df[group_col] == group1][value_col]
        group2_data = df[df[group_col] == group2][value_col]

        initial_count_group1 = len(group1_data)
        initial_count_group2 = len(group2_data)

        # Remove NaN values
        group1_data = group1_data.dropna()
        group2_data = group2_data.dropna()

        remaining_count_group1 = len(group1_data)
        remaining_count_group2 = len(group2_data)

        # Show results
        st.write(f"**Group 1 ({group1}):** Total number of data points: {initial_count_group1}, without NaN: {remaining_count_group1}")
        st.write(f"**Group 2 ({group2}):** Total number of data points: {initial_count_group2}, without NaN: {remaining_count_group2}")

        # Apply logarithmic scaling if activated
        if apply_log_scale:
            group1_data = np.log1p(group1_data)
            group2_data = np.log1p(group2_data)

        if not group1_data.empty and not group2_data.empty:
            # Perform t-test
            t_stat, p_value = stats.ttest_ind(group1_data, group2_data)
            st.markdown(f"**t-Statistic:** {t_stat}")
            st.markdown(f"**p-Value:** {p_value}")
            
            # Create boxplot
            filtered_df = df[df[group_col].isin([group1, group2])]
            fig, ax = plt.subplots()
            sns.boxplot(x=filtered_df[group_col], y=filtered_df[value_col], ax=ax, palette="Set2")
            ax.set_title(f"Boxplot for {group1} vs. {group2}")

            # Logarithmic scaling in the plot if activated
            if apply_log_scale:
                ax.set_yscale('log')

            st.pyplot(fig)

            # Analysis of outliers
            outliers_group1 = group1_data[group1_data > group1_data.quantile(0.75) + 1.5 * (group1_data.quantile(0.75) - group1_data.quantile(0.25))]
            outliers_group2 = group2_data[group2_data > group2_data.quantile(0.75) + 1.5 * (group2_data.quantile(0.75) - group2_data.quantile(0.25))]

            st.write("**Outlier Analysis:**")
            if not outliers_group1.empty:
                st.write(f"In group 1 ({group1}) there are {len(outliers_group1)} outliers.")
            else:
                st.write(f"In group 1 ({group1}) there are no significant outliers.")

            if not outliers_group2.empty:
                st.write(f"In group 2 ({group2}) there are {len(outliers_group2)} outliers.")
            else:
                st.write(f"In group 2 ({group2}) there are no significant outliers.")
        else:
            st.error("One or both groups contain no data after removing NaN values.")


# Function for the ANOVA test
def anova_test(df, numerical_columns, categorical_columns):
    group_col = st.selectbox("Choose the group variable:", categorical_columns)
    value_col = st.selectbox("Choose the value variable:", numerical_columns)

    if st.button("Perform ANOVA"):
        df_clean = df[[group_col, value_col]].dropna()

        # Determine group sizes
        group_sizes = df_clean.groupby(group_col).size()
        valid_groups = group_sizes[group_sizes >= 2].index
        df_filtered = df_clean[df_clean[group_col].isin(valid_groups)]

        if len(valid_groups) < 2:
            st.error("After removing small groups, there are not enough groups left for the ANOVA test.")
        else:
            grouped_data = [group[value_col].values for name, group in df_filtered.groupby(group_col)]
            try:
                anova_result = stats.f_oneway(*grouped_data)
                st.markdown(f"**F-Value:** {anova_result.statistic}")
                st.markdown(f"**p-Value:** {anova_result.pvalue}")

                # Boxplot to visualize the results
                fig, ax = plt.subplots(figsize=(10, 6))
                sns.boxplot(x=group_col, y=value_col, data=df_filtered, ax=ax)
                plt.xticks(rotation=90)
                st.pyplot(fig)

                if anova_result.pvalue < 0.05:
                    st.write("The ANOVA test is significant. Tukey's HSD test will be performed.")
                    try:
                        tukey = pairwise_tukeyhsd(endog=df_filtered[value_col], groups=df_filtered[group_col], alpha=0.05)
                        st.pyplot(tukey.plot_simultaneous())

                        # Show Tukey HSD results
                        tukey_results_df = pd.DataFrame(data=tukey.summary().data[1:], columns=tukey.summary().data[0])
                        st.write("Results of the Tukey HSD test:")
                        st.dataframe(tukey_results_df, height=400)

                        # Download option for results
                        csv = tukey_results_df.to_csv(index=False)
                        st.download_button(label="Download Tukey HSD results as CSV", data=csv, file_name='tukey_hsd_results.csv', mime='text/csv')

                    except Exception as e:
                        st.error(f"An error occurred during Tukey's HSD test: {str(e)}")

            except ValueError as e:
                st.error(f"An error occurred: {str(e)}.")


# Function for the Chi-square test
def chi_square_test(df, categorical_columns):
    cat_var1 = st.selectbox("Choose the first group variable:", categorical_columns)
    cat_var2 = st.selectbox("Choose the second group variable:", categorical_columns)
    
    if st.button("Perform Chi-square test"):
        # Remove NaN values from the selected variables
        df_clean = df[[cat_var1, cat_var2]].dropna()

        # Filter the top 10 categories for both variables
        top_cat_var1 = df_clean[cat_var1].value_counts().nlargest(10).index
        top_cat_var2 = df_clean[cat_var2].value_counts().nlargest(10).index
        df_filtered = df_clean[df_clean[cat_var1].isin(top_cat_var1) & df_clean[cat_var2].isin(top_cat_var2)]

        # Create the contingency table
        try:
            contingency_table = pd.crosstab(df_filtered[cat_var1], df_filtered[cat_var2])

            # Check if the contingency table is valid
            if contingency_table.empty or contingency_table.shape[0] < 2 or contingency_table.shape[1] < 2:
                st.error("The contingency table is invalid. Check the variables.")
            else:
                chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
                st.markdown(f"**Chi-square:** {chi2}")
                st.markdown(f"**p-Value:** {p}")
                
                # Visualize the contingency table as a heatmap
                st.write("**Heatmap of the contingency table:**")
                fig, ax = plt.subplots(figsize=(12, 10))  # Larger display
                sns.heatmap(contingency_table, annot=False, cmap="YlGnBu", ax=ax)
                ax.set_title(f"Heatmap of the contingency table: {cat_var1} vs. {cat_var2} top 10")
                plt.xticks(rotation=90)
                st.pyplot(fig)

        except ValueError as e:
            st.error(f"An error occurred: {str(e)}.")


# Function for regression analysis
def regression_analysis(df, numerical_columns):
    reg_type = st.selectbox("Choose the type of regression:", ["Linear regression", "Logistic regression", "Multivariate regression"])

    # Linear regression
    if reg_type == "Linear regression":
        st.markdown("""
        **Linear regression:**
        This method models the relationship between a dependent variable and one or more independent variables.
        It is used to predict continuous values.
        """)
        linear_regression(df, numerical_columns)

    # Logistic regression
    elif reg_type == "Logistic regression":
        st.markdown("""
        **Logistic regression:**
        This method is used to model binary outcomes (e.g., Yes/No). It is particularly useful for classification problems.
        """)
        logistic_regression(df, numerical_columns)

    # Multivariate regression
    elif reg_type == "Multivariate regression":
        st.markdown("""
        **Multivariate regression:**
        Multivariate regression analyzes multiple dependent variables simultaneously to model their relationship with the independent variables.
        """)
        multivariate_regression(df, numerical_columns)


# Function for linear regression with separate models
def linear_regression(df, numerical_columns):
    st.write("**Correlation matrix of numerical variables:**")
    corr_matrix = df[numerical_columns].corr()
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', ax=ax)
    st.pyplot(fig)

    dependent_var = st.selectbox("Choose the dependent variable:", numerical_columns)
    independent_vars = st.multiselect("Choose the independent variables:", numerical_columns)

    if independent_vars:
        if st.button("Perform regression"):
            X = df[independent_vars].dropna()
            y = df[dependent_var].loc[X.index]
            y = y.dropna()
            X = X.loc[y.index]

            if y.isnull().values.any():
                st.error("The dependent variable still contains missing values. Please clean the data.")
            else:
                # Full model with all independent variables
                model = LinearRegression()
                model.fit(X, y)

                st.markdown("**Regression coefficients:**")
                for var, coef in zip(independent_vars, model.coef_):
                    st.write(f"- {var}: {coef}")
                st.write(f"**Intercept:** {model.intercept_}")

                # Separate models and scatterplots for each variable
                for var in independent_vars:
                    X_single_var = X[[var]]  # Use only the current independent variable
                    model_single = LinearRegression()
                    model_single.fit(X_single_var, y)

                    fig, ax = plt.subplots()
                    ax.scatter(X[var], y, edgecolor='none', facecolors='blue', s=5, label='Data points')

                    # Regression line based on the single model
                    ax.plot(X[var], model_single.predict(X_single_var), color='red', label='Regression line')
                    ax.set_xlabel(var)
                    ax.set_ylabel(dependent_var)
                    ax.set_title(f"{dependent_var} vs {var}")
                    ax.legend()
                    st.pyplot(fig)


# Function for logistic regression
def logistic_regression(df, numerical_columns):
    dependent_var = st.selectbox("Choose the dependent variable (binary):", numerical_columns)
    independent_vars = st.multiselect("Choose the independent variables:", numerical_columns)

    if independent_vars:
        if st.button("Perform logistic regression"):
            X = df[independent_vars].dropna()
            y = df[dependent_var].loc[X.index].dropna()
            X = X.loc[y.index]

            # Check if the dependent variable is binary
            unique_values = y.unique()
            if len(unique_values) != 2:
                st.error("The dependent variable must be binary (e.g., 0 and 1).")
            else:
                model = LogisticRegression()
                model.fit(X, y)

                st.write("**Logistic regression coefficients:**")
                for var, coef in zip(independent_vars, model.coef_[0]):
                    st.write(f"- {var}: {coef}")
                st.write(f"**Intercept:** {model.intercept_[0]}")

                # Visualization of the logistic function for each independent variable
                for var in independent_vars:
                    fig, ax = plt.subplots()

                    # Scatterplot of actual data points
                    ax.scatter(X[var], y, label='Data points')

                    # Create range of values for the current variable var
                    x_range = np.linspace(X[var].min(), X[var].max(), 300).reshape(-1, 1)

                    # Calculate means for all other variables and use them
                    X_copy = pd.DataFrame(np.tile(X.mean().values, (300, 1)), columns=X.columns)
                    X_copy[var] = x_range.flatten()  # Vary the current variable var

                    # Create predictions based on all variables
                    y_prob = model.predict_proba(X_copy)[:, 1]

                    # Plot logistic function (probabilities)
                    ax.plot(x_range, y_prob, color='red', label='Logistic function')
                    ax.set_xlabel(var)
                    ax.set_ylabel(f'Probability ({dependent_var})')
                    ax.set_title(f'Logistic regression: {dependent_var} vs {var}')
                    ax.legend()
                    st.pyplot(fig)


# Function for multivariate regression
def multivariate_regression(df, numerical_columns):
    dependent_vars = st.multiselect("**Choose the dependent variables (multiple):**", numerical_columns)
    independent_vars = st.multiselect("**Choose the independent variables:**", numerical_columns)

    if dependent_vars and independent_vars:
        if st.button("Perform multivariate regression"):
            X = df[independent_vars].dropna()
            Y = df[dependent_vars].loc[X.index].dropna()
            X = X.loc[Y.index]

            if X.shape[1] != len(independent_vars) or Y.shape[1] != len(dependent_vars):
                st.error("The number of independent or dependent variables does not match.")
                return

            model = LinearRegression()
            model.fit(X, Y)

            st.write("**Multivariate regression coefficients:**")
            for i, dep_var in enumerate(dependent_vars):
                st.write(f"\nFor the dependent variable: **{dep_var}**")
                st.write(f"Intercept: {model.intercept_[i]}")
                for var, coef in zip(independent_vars, model.coef_[i]):
                    st.write(f"- {var}: {coef}")

            # Visualization of the regression results
            for dep_var in dependent_vars:
                for var in independent_vars:
                    fig, ax = plt.subplots()
                    ax.scatter(X[var], Y[dep_var], label='Data points')

                    # Ensure that the prediction is made with all variables for the scatterplot
                    x_range = np.linspace(X[var].min(), X[var].max(), 300).reshape(-1, 1)

                    # Create a copy of X with the mean value for all variables except the current variable
                    X_copy = pd.DataFrame(np.tile(X.mean().values, (300, 1)), columns=X.columns)
                    X_copy[var] = x_range.flatten()

                    y_pred = model.predict(X_copy)

                    ax.plot(x_range, y_pred[:, dependent_vars.index(dep_var)], color='red', label='Regression line')
                    ax.set_xlabel(var)
                    ax.set_ylabel(dep_var)
                    ax.set_title(f'Multivariate regression: {dep_var} vs {var}')
                    ax.legend()
                    st.plotly_chart(fig)


# Function for time series analysis with yearly averages
def perform_time_series_analysis(df, time_var, value_var):
    # Convert the time variable to date format and remove invalid data
    df[time_var] = pd.to_datetime(df[time_var], errors='coerce')
    df = df.dropna(subset=[time_var])

    if df.empty:
        st.error("**Error:** The time variable has an incorrect format.")
    else:
        # Create a new column 'year' to allow yearly grouping
        df['year'] = df[time_var].dt.year
        yearly_avg = df.groupby('year')[value_var].mean().reset_index()

        # Determine the minimum and maximum values
        y_min = yearly_avg[value_var].min()
        y_max = yearly_avg[value_var].max()
        y_range = y_max - y_min
        y_buffer = y_range * 0.05

        # Calculate the overall average
        overall_avg = df[value_var].mean()

        # Create the plot
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(yearly_avg['year'], yearly_avg[value_var], marker='o', label='Yearly average')
        ax.axhline(overall_avg, color='red', linestyle='--', label=f'Overall average: {overall_avg:.2f}')
        ax.set_title(f'Average {value_var} per year')
        ax.set_xlabel('Year')
        ax.set_ylabel(f'Average {value_var}')
        ax.set_ylim(y_min - y_buffer, y_max + y_buffer)

        # Show the overall average as text in the plot
        ax.text(yearly_avg['year'].max() - (yearly_avg['year'].max() - yearly_avg['year'].min()) * 0.05, 
                overall_avg + y_buffer, 
                f'{overall_avg:.2f}', color='red', ha='right', va='center')

        # Display yearly average values
        for i in range(len(yearly_avg)):
            if i % 2 == 0:
                ax.text(yearly_avg['year'][i], yearly_avg[value_var][i] + y_buffer/2, 
                        f'{yearly_avg[value_var][i]:.2f}', color='blue', ha='center', va='bottom')
            else:
                ax.text(yearly_avg['year'][i], yearly_avg[value_var][i] - y_buffer/2, 
                        f'{yearly_avg[value_var][i]:.2f}', color='blue', ha='center', va='top')

        plt.xticks(rotation=45)
        ax.legend()
        st.pyplot(fig)

        # Output calculated statistics
        st.write(f"**Standard deviation:** {df[value_var].std():.2f}")
        st.write(f"**Variance:** {df[value_var].var():.2f}")
        st.write(f"**Minimum {value_var}:** {y_min:.2f} in year {yearly_avg.loc[yearly_avg[value_var].idxmin(), 'year']}")
        st.write(f"**Maximum {value_var}:** {y_max:.2f} in year {yearly_avg.loc[yearly_avg[value_var].idxmax(), 'year']}")


# Function for clustering methods
def clustering_methods(df, numerical_columns):
    cluster_method = st.selectbox("Choose the clustering method:", ["k-Means", "Hierarchical Clustering", "DBSCAN"])
    
    # Show explanation based on the selected method
    if cluster_method == "k-Means":
        st.markdown("""
        **k-Means Clustering:**
        This method groups data points into k clusters by minimizing the distance between the points. 
        It is often used to find similarities in the data.
        """)
    elif cluster_method == "Hierarchical Clustering":
        st.markdown("""
        **Hierarchical Clustering:**
        Hierarchical clustering creates a hierarchy of clusters that can be represented by a dendrogram structure. 
        This allows understanding the structure of the data at different levels.
        """)
    elif cluster_method == "DBSCAN":
        st.markdown("""
        **DBSCAN (Density-Based Clustering):**
        DBSCAN groups points based on the density of their surroundings. It is particularly useful for identifying clusters of arbitrary shape 
        and can detect outliers (noise).
        """)

    # Selection of variables for clustering
    selected_vars = st.multiselect("Choose the variables for clustering:", numerical_columns)

    if selected_vars:
        # Parameters for k-Means or hierarchical clustering
        if cluster_method == "k-Means" or cluster_method == "Hierarchical Clustering":
            n_clusters = st.slider("**Number of clusters**", 2, 10, 3)
        # Parameters for DBSCAN
        elif cluster_method == "DBSCAN":
            eps = st.slider("Choose the radius (eps):", 0.1, 5.0, 0.5)
            min_samples = st.slider("Choose the minimum number of points per cluster (min_samples):", 1, 10, 5)

        # Button to start the clustering process
        if st.button("Perform clustering"):
            X = df[selected_vars].dropna()  # Remove rows with missing values
            if cluster_method == "k-Means":
                perform_kmeans(X, n_clusters)
            elif cluster_method == "Hierarchical Clustering":
                perform_hierarchical_clustering(X, n_clusters)
            elif cluster_method == "DBSCAN":
                perform_dbscan(X, eps, min_samples)
    else:
        st.error("Please select at least two variables!")


# Function for k-Means Clustering
def perform_kmeans(X, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    X['Cluster'] = kmeans.fit_predict(X)
    
    visualize_clusters(X, 'k-Means Clustering')


# Function for hierarchical clustering
def perform_hierarchical_clustering(X, n_clusters):
    hierarchical_clustering = AgglomerativeClustering(n_clusters=n_clusters)
    X['Cluster'] = hierarchical_clustering.fit_predict(X)
       
    visualize_clusters(X, 'Hierarchical Clustering')


# Function for DBSCAN
def perform_dbscan(X, eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    X['Cluster'] = dbscan.fit_predict(X)
    
    visualize_clusters(X, 'DBSCAN Clustering')


# Function to visualize clusters with PCA
def visualize_clusters(X, title):
    # Ensure that we have enough data points and variables
    num_samples, num_features = X.shape
    
    # Take the minimum of samples and features to adjust the number of components for PCA
    n_components = min(num_samples, num_features, 2)  # max 2 components, but fewer if not enough data

    # If there are fewer than 2 variables or data points, PCA cannot be performed
    if n_components < 2:
        st.error("Not enough data points or variables to perform PCA.")
        return

    pca = PCA(n_components=n_components)
    
    try:
        X_pca = pca.fit_transform(X.drop(columns=['Cluster']))
        
        fig, ax = plt.subplots(figsize=(10, 6))
        scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=X['Cluster'], cmap='tab10', s=25, alpha=0.4)
        
        ax.set_title(title)
        ax.set_xlabel(f'PCA 1' if n_components >= 1 else '')
        ax.set_ylabel(f'PCA 2' if n_components == 2 else '')

        # Retrieve cluster assignments and their counts
        cluster_counts = X['Cluster'].value_counts()  

        # Create legend
        legend_labels = [f"Cluster {int(cluster)} ({count} points)" for cluster, count in cluster_counts.items()]
        legend1 = ax.legend(handles=scatter.legend_elements()[0], labels=legend_labels)
        ax.add_artist(legend1)

        st.pyplot(fig)

        # Show the average values of the variables for each cluster
        st.write(f"**Average values per cluster:**")
        cluster_means = X.groupby('Cluster').mean()
        st.dataframe(cluster_means)

    except ValueError as e:
        st.error(f"**Error:** Not enough variables were selected.")


# Function to communicate with the LLM
def query_llm_via_cli(input_text):
    """Sends the question and context to the LLM and receives a response"""
    try:
        process = subprocess.Popen(
            ["ollama", "run", "llama3.1"],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8',
            errors='ignore',
            bufsize=1
        )
        stdout, stderr = process.communicate(input=f"{input_text}\n", timeout=40)

        if process.returncode != 0:
            return f"Error in the model request: {stderr.strip()}"

        response = re.sub(r'\x1b\[.*?m', '', stdout)
        return extract_relevant_answer(response)

    except subprocess.TimeoutExpired:
        process.kill()
        return "Timeout for the model request"
    except Exception as e:
        return f"An unexpected error has occurred: {str(e)}"

def extract_relevant_answer(full_response):
    response_lines = full_response.splitlines()
    if response_lines:
        return "\n".join(response_lines).strip()
    return "No answer received"


# Main function to start the app
def main():
    st.title("ASCVIT V1")

    # Sidebar for file upload
    st.sidebar.title("Settings")
    uploaded_file = st.sidebar.file_uploader("**Upload your data file**", type=["csv", "xlsx"])

    # Check if a file has been uploaded
    if uploaded_file:
        # Read file (either CSV or Excel)
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        else:
            df = pd.read_excel(uploaded_file)

        # Reset numeric and categorical columns based on the new record
        numerical_columns = df.select_dtypes(include=np.number).columns.tolist()
        categorical_columns = df.select_dtypes(include='object').columns.tolist()

        # Update the session state variables only when a new file has been uploaded
        if 'last_uploaded_file' not in st.session_state or st.session_state['last_uploaded_file'] != uploaded_file.name:
            st.session_state['numerical_columns'] = numerical_columns
            st.session_state['categorical_columns'] = categorical_columns
            st.session_state['last_uploaded_file'] = uploaded_file.name  # Speichere den Dateinamen

        # Show data only when the button is clicked
        if st.sidebar.button("Data Overview"):
            st.session_state['show_data'] = True
            st.session_state['show_analysis'] = None  # Disable analysis display

        # Analysis methods as buttons in the sidebar
        if st.sidebar.button("Descriptive Statistics"):
            st.session_state['show_data'] = False
            st.session_state['show_analysis'] = 'Descriptive Statistics'

        if st.sidebar.button("Hypothesis Tests"):
            st.session_state['show_data'] = False
            st.session_state['show_analysis'] = 'Hypothesis Tests'

        if st.sidebar.button("Regression Analysis"):
            st.session_state['show_data'] = False
            st.session_state['show_analysis'] = 'Regression Analysis'

        if st.sidebar.button("Time Series Analysis"):
            st.session_state['show_data'] = False
            st.session_state['show_analysis'] = 'Time Series Analysis'

        if st.sidebar.button("Clustering Methods"):
            st.session_state['show_data'] = False
            st.session_state['show_analysis'] = 'Clustering Methods'

        # Show data or analysis depending on the selection
        if 'show_data' in st.session_state and st.session_state['show_data']:
            st.header("Data Overview")
            st.write("**Data Preview:**")
            st.dataframe(df.head())
            display_data_info(df)

        if 'show_analysis' in st.session_state and st.session_state['show_analysis']:
            if st.session_state['show_analysis'] == 'Descriptive Statistics':
                st.header("Descriptive Statistics")
                descriptive_statistics(df, numerical_columns)

            if st.session_state['show_analysis'] == 'Hypothesis Tests':
                st.header("Hypothesis Tests")
                hypothesis_testing(df, numerical_columns, categorical_columns)

            if st.session_state['show_analysis'] == 'Regression Analysis':
                st.header("Regression Analysis")
                regression_analysis(df, numerical_columns)

            if st.session_state['show_analysis'] == 'Time Series Analysis':
                st.header("Time Series Analysis")
                time_var = st.selectbox("Choose the time variable:", df.columns)
                value_var = st.selectbox("Choose the value variable:", numerical_columns)
                
                # Explanation is shown immediately
                st.markdown("""
                **Time Series Analysis:**
                This method examines how a variable changes over time. 
                It helps to identify trends, seasonal patterns, and other time-dependent phenomena.
                """)

                if st.button("Perform time series analysis"):
                    if time_var and value_var:
                        perform_time_series_analysis(df, time_var, value_var)
                    else:
                        st.error("Please choose both a time variable and a value variable.")

            if st.session_state['show_analysis'] == 'Clustering Methods':
                st.header("Clustering Methods")
                clustering_methods(df, numerical_columns)

    else:
        st.sidebar.info("Please upload a data file to get started.")

if __name__ == "__main__":
    main()
